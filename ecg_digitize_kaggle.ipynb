{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# ECG Printout Digitization (Kaggle-ready)\n\nThis notebook rebuilds the inference pipeline described in **Combining Hough Transform and Deep Learning Approaches to Reconstruct ECG Signals From Printouts**. It performs:\n\n1. Rotation correction via Hough Transform.\n2. ECG trace segmentation with **nnU-Net v2**.\n3. Lead-wise vectorization to reconstruct 12-lead signals and save them as WFDB/NumPy outputs.\n\nFill in the CONFIG cell (paths to your input images and pretrained nnU-Net fold) and run all cells. All outputs are written to `/kaggle/working/` so the notebook works offline within Kaggle."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# %%capture --no-stdout\n!pip install -q opencv-python torch torchvision nnunetv2 wfdb tqdm matplotlib pillow numpy nibabel\n\nimport importlib, sys\npackages = {\n    \"cv2\": \"opencv-python\",\n    \"torch\": \"torch\",\n    \"nnunetv2\": \"nnunetv2\",\n    \"wfdb\": \"wfdb\",\n    \"tqdm\": \"tqdm\",\n    \"PIL\": \"pillow\",\n    \"numpy\": \"numpy\",\n    \"matplotlib\": \"matplotlib\",\n    \"nibabel\": \"nibabel\"\n}\nfor module, pkg in packages.items():\n    try:\n        mod = importlib.import_module(module if module != \"PIL\" else \"PIL\")\n        version = getattr(mod, \"__version__\", getattr(mod, \"VERSION\", None))\n        print(f\"{pkg}: {version}\")\n    except Exception as e:\n        print(f\"{pkg}: not available ({e})\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## CONFIG: edit input paths and defaults\nUpdate the following cell with your Kaggle Dataset paths. Defaults are aligned with `config.py` and the provided nnU-Net fold layout."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# User-editable configuration\nDEBUG = True\nDEBUG_MAX_IMAGES = 2\nSAVE_OVERLAY = False\n\n# Input/output paths\nINPUT_IMAGES_DIR = \"/kaggle/input/<your-ecg-images>/\"\nOUTPUT_DIR = \"/kaggle/working/digitized/\"\n\n# nnU-Net pretrained model placement\nMODEL_FOLD_DIR_DEFAULT = \"/kaggle/input/nnunet-ecg-dataset500-fold0-best-min/Dataset500_Signals/nnUNetTrainer__nnUNetPlans__2d/fold_0\"\nNNUNET_RESULTS_ROOT_DEFAULT = \"/kaggle/input/nnunet-ecg-dataset500-fold0-best-min\"\nNNUNET_DATASET_NAME = \"Dataset500_Signals\"\nNNUNET_TRAINER = \"nnUNetTrainer\"\nNNUNET_CONFIG = \"2d\"\nNNUNET_PLANS = \"nnUNetPlans\"\nNNUNET_FOLD = \"0\"\nCHECKPOINT_NAME = \"checkpoint_best.pth\"  # fallback to checkpoint_final.pth if missing\n\n# Signal configuration (from config.py)\nIMAGE_TYPE = \"png\"\nFREQUENCY = 500\nDATASET_NAME = \"Dataset500_Signals\"\nLONG_SIGNAL_LENGTH_SEC = 10\nSHORT_SIGNAL_LENGTH_SEC = 2.5\nSIGNAL_UNITS = \"mV\"\nFMT = \"16\"\nADC_GAIN = 1000.0\nBASELINE = 0\n\nLEAD_LABEL_MAPPING = {\n  \"I\":1,\"II\":2,\"III\":3,\"aVR\":4,\"aVL\":5,\"aVF\":6,\n  \"V1\":7,\"V2\":8,\"V3\":9,\"V4\":10,\"V5\":11,\"V6\":12\n}\n\nY_SHIFT_RATIO = {\n  \"I\": 12.6/21.59, \"II\": 9/21.59, \"III\": 5.4/21.59,\n  \"aVR\": 12.6/21.59, \"aVL\": 9/21.59, \"aVF\": 5.4/21.59,\n  \"V1\": 12.59/21.59, \"V2\": 9/21.59, \"V3\": 5.4/21.59,\n  \"V4\": 12.59/21.59, \"V5\": 9/21.59, \"V6\": 5.4/21.59,\n  \"full\": 2.1/21.59\n}\n\n# Hough/segmentation settings\nHOUGH_DEGREE_WINDOW = 5\nPARALLELISM_COUNT = 5\nPARALLELISM_WINDOW = 2\nCANNY_THRESHOLDS = (50, 150)\nSEC_PER_PAPER_SECOND = 25  # mm/s\nMV_PER_MM = 0.1  # 10 mm/mV -> 0.1 mV per mm -> mm_per_pixel/10\n\n# Derived writable paths\nimport os\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nTMP_ROTATED_DIR = os.path.join(OUTPUT_DIR, \"rotated_images\")\nTMP_MASKS_DIR = os.path.join(OUTPUT_DIR, \"nnunet_masks\")\nfor d in [TMP_ROTATED_DIR, TMP_MASKS_DIR]:\n    os.makedirs(d, exist_ok=True)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Utilities\nHelper functions for path inspection and logging."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os, json, glob, shutil, math, subprocess, warnings\nfrom typing import List, Tuple, Dict, Optional\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn.functional as F\nimport nibabel as nib\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport wfdb\n\n\ndef print_directory_tree(base: str, max_depth: int = 2):\n    base_path = Path(base)\n    print(f\"\nDirectory tree for {base_path} (depth {max_depth}):\")\n    for path in sorted(base_path.rglob('*')):\n        depth = len(path.relative_to(base_path).parts)\n        if depth > max_depth:\n            continue\n        prefix = '  ' * depth\n        print(f\"{prefix}{path.name}{'/' if path.is_dir() else ''}\")\n\n\ndef check_paths():\n    print(f\"INPUT_IMAGES_DIR exists: {os.path.exists(INPUT_IMAGES_DIR)} -> {INPUT_IMAGES_DIR}\")\n    print(f\"MODEL_FOLD_DIR_DEFAULT exists: {os.path.exists(MODEL_FOLD_DIR_DEFAULT)} -> {MODEL_FOLD_DIR_DEFAULT}\")\n    print(f\"NNUNET_RESULTS_ROOT_DEFAULT exists: {os.path.exists(NNUNET_RESULTS_ROOT_DEFAULT)} -> {NNUNET_RESULTS_ROOT_DEFAULT}\")\n    print_directory_tree(NNUNET_RESULTS_ROOT_DEFAULT, max_depth=3)\n    print_directory_tree(MODEL_FOLD_DIR_DEFAULT, max_depth=2)\n\ncheck_paths()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Rotation correction (Hough transform)\nFollows the paper: detect near-horizontal lines with Canny + HoughLines, keep parallel lines, and rotate by the median angle. If no reliable lines are found, fall back to 0°."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def get_lines(gray_img: np.ndarray, canny_thresholds: Tuple[int, int] = CANNY_THRESHOLDS):\n    edges = cv2.Canny(gray_img, *canny_thresholds)\n    lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=150)\n    return lines\n\n\ndef filter_lines(lines, degree_window=HOUGH_DEGREE_WINDOW, parallelism_count=PARALLELISM_COUNT, parallelism_window=PARALLELISM_WINDOW):\n    if lines is None:\n        return []\n    filtered = []\n    for line in lines:\n        rho, theta = line[0]\n        degree = np.degrees(theta) - 90  # horizontal around 0\n        if abs(degree) <= degree_window:\n            filtered.append(degree)\n    if len(filtered) < parallelism_count:\n        return []\n    filtered_sorted = sorted(filtered)\n    grouped = []\n    for d in filtered_sorted:\n        if not grouped or abs(grouped[-1][-1] - d) > parallelism_window:\n            grouped.append([d])\n        else:\n            grouped[-1].append(d)\n    best_group = max(grouped, key=len) if grouped else []\n    return best_group\n\n\ndef get_median_degrees(filtered_degrees: List[float]) -> float:\n    return float(np.median(filtered_degrees)) if filtered_degrees else 0.0\n\n\ndef get_rotation_angle(gray_img: np.ndarray) -> float:\n    lines = get_lines(gray_img)\n    filtered = filter_lines(lines)\n    angle = get_median_degrees(filtered)\n    if not filtered:\n        warnings.warn(\"Hough transform failed, using 0 degrees.\")\n    return angle\n\n\ndef rotate_image(img: np.ndarray, angle: float) -> np.ndarray:\n    h, w = img.shape[:2]\n    center = (w // 2, h // 2)\n    matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n    rotated = cv2.warpAffine(img, matrix, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)\n    return rotated\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## nnU-Net v2 inference\nAdapts `digitize.py` to Kaggle: sets `nnUNet_results` from the provided root, resolves the checkpoint (best → final), and uses fold 0 by default. Predictions are batched by first writing rotated images to a temp folder."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def resolve_nnunet_paths(model_fold_dir: str, results_root: str) -> Tuple[str, str]:\n    resolved_results = results_root\n    if os.path.isdir(os.path.join(model_fold_dir, \"nnUNet_results\")):\n        resolved_results = os.path.join(model_fold_dir, \"nnUNet_results\")\n    os.environ[\"nnUNet_results\"] = resolved_results\n    os.environ[\"nnUNet_raw\"] = os.path.join(OUTPUT_DIR, \"nnUNet_raw\")\n    os.environ[\"nnUNet_preprocessed\"] = os.path.join(OUTPUT_DIR, \"nnUNet_preprocessed\")\n    for env_path in [os.environ[\"nnUNet_raw\"], os.environ[\"nnUNet_preprocessed\"]]:\n        os.makedirs(env_path, exist_ok=True)\n    print(f\"nnUNet_results -> {resolved_results}\")\n    print(f\"nnUNet_raw -> {os.environ['nnUNet_raw']}\")\n    print(f\"nnUNet_preprocessed -> {os.environ['nnUNet_preprocessed']}\")\n    return resolved_results, model_fold_dir\n\n\ndef resolve_checkpoint(model_fold_dir: str, checkpoint_name: str) -> str:\n    best_path = os.path.join(model_fold_dir, checkpoint_name)\n    final_path = os.path.join(model_fold_dir, \"checkpoint_final.pth\")\n    if os.path.exists(best_path):\n        return checkpoint_name\n    if os.path.exists(final_path):\n        return \"checkpoint_final.pth\"\n    raise FileNotFoundError(f\"No checkpoint found in {model_fold_dir}. Checked {best_path} and {final_path}.\")\n\n\ndef run_nnunet_predict(rotated_dir: str, masks_dir: str, model_fold_dir: str, results_root: str, device: str = \"gpu\"):\n    resolve_nnunet_paths(model_fold_dir, results_root)\n    checkpoint_flag = resolve_checkpoint(model_fold_dir, CHECKPOINT_NAME)\n    cmd = [\n        \"nnUNetv2_predict\",\n        \"-d\", NNUNET_DATASET_NAME,\n        \"-i\", rotated_dir,\n        \"-o\", masks_dir,\n        \"-f\", NNUNET_FOLD,\n        \"-tr\", NNUNET_TRAINER,\n        \"-c\", NNUNET_CONFIG,\n        \"-p\", NNUNET_PLANS,\n        \"-chk\", checkpoint_flag\n    ]\n    if device.lower() == \"cpu\":\n        cmd += [\"-device\", \"cpu\", \"--verbose\"]\n    print(\"Running nnUNetv2_predict:\", \" \".join(cmd))\n    result = subprocess.run(cmd, check=True, text=True, capture_output=True)\n    print(result.stdout)\n    if result.stderr:\n        print(result.stderr)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Mask cutting and vectorization\nRecreate `cut_to_mask`, `cut_binary`, and the column-wise vectorization. Each lead mask is cropped by bounding box; missing leads are filled with NaN. Temporal scaling uses the paper's constants (25 mm/s, 10 mm/mV)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def cut_binary(mask: np.ndarray, lead_id: int) -> Tuple[Optional[np.ndarray], Optional[Tuple[int,int]]]:\n    binary = (mask == lead_id).astype(np.uint8)\n    coords = cv2.findNonZero(binary)\n    if coords is None:\n        return None, None\n    x, y, w, h = cv2.boundingRect(coords)\n    cropped = binary[y:y+h, x:x+w]\n    return cropped, (y, x)\n\n\ndef cut_to_mask(mask: np.ndarray, lead_mapping: Dict[str, int]):\n    lead_masks = {}\n    for lead, idx in lead_mapping.items():\n        cropped, origin = cut_binary(mask, idx)\n        lead_masks[lead] = {\"mask\": cropped, \"origin\": origin}\n    return lead_masks\n\n\ndef estimate_scaling(mask_width: int) -> Tuple[float, float]:\n    x_pixel_list = [mask_width]\n    median = np.median(x_pixel_list)\n    mean_below_2x = np.mean([v for v in x_pixel_list if v < 2 * median])\n    sec_per_pixel = SHORT_SIGNAL_LENGTH_SEC / mean_below_2x\n    mm_per_pixel = SEC_PER_PAPER_SECOND * sec_per_pixel\n    mV_per_pixel = mm_per_pixel / 10\n    return sec_per_pixel, mV_per_pixel\n\n\ndef vectorise_mask(mask_info: Dict[str, Dict], image_height: int) -> Dict[str, np.ndarray]:\n    lead_signals = {}\n    for lead, info in mask_info.items():\n        lead_mask, origin = info.get(\"mask\"), info.get(\"origin\")\n        if lead_mask is None or origin is None:\n            lead_signals[lead] = None\n            continue\n        y1, x1 = origin\n        h, w = lead_mask.shape\n        non_zero = torch.nonzero(torch.tensor(lead_mask))\n        if non_zero.numel() == 0:\n            lead_signals[lead] = None\n            continue\n        non_zero_y_mean = non_zero[:, 0].float().mean()\n        sec_per_pixel, mV_per_pixel = estimate_scaling(w)\n        total_seconds_from_mask = sec_per_pixel * w\n        if total_seconds_from_mask > LONG_SIGNAL_LENGTH_SEC / 2:\n            total_seconds = LONG_SIGNAL_LENGTH_SEC\n            y_shift_ratio = Y_SHIFT_RATIO[\"full\"]\n        else:\n            total_seconds = SHORT_SIGNAL_LENGTH_SEC\n            y_shift_ratio = Y_SHIFT_RATIO[lead]\n        values_needed = int(total_seconds * FREQUENCY)\n        signal_cropped_shifted = (1 - y_shift_ratio) * image_height - y1\n        predicted_signal = (signal_cropped_shifted - non_zero_y_mean) * mV_per_pixel\n        signal = torch.full((w,), predicted_signal, dtype=torch.float32)\n        resampled = F.interpolate(signal.unsqueeze(0).unsqueeze(0), size=values_needed, mode=\"linear\", align_corners=False).squeeze().numpy()\n        lead_signals[lead] = resampled\n    return lead_signals\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Saving outputs\nWrite WFDB records (or NumPy) with consistent metadata. Missing leads fall back to zeros for WFDB compatibility."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def save_wfdb(record_id: str, lead_signals: Dict[str, np.ndarray], output_dir: str):\n    sig_names = list(LEAD_LABEL_MAPPING.keys())\n    max_len = max(len(v) for v in lead_signals.values() if v is not None)\n    stacked = []\n    for lead in sig_names:\n        sig = lead_signals.get(lead)\n        if sig is None:\n            sig = np.zeros(max_len)\n        if len(sig) != max_len:\n            sig = F.interpolate(torch.tensor(sig).unsqueeze(0).unsqueeze(0), size=max_len, mode=\"linear\", align_corners=False).squeeze().numpy()\n        stacked.append(sig)\n    stacked = np.vstack(stacked).T\n    wfdb.wrsamp(\n        os.path.join(output_dir, record_id),\n        fs=FREQUENCY,\n        units=[SIGNAL_UNITS] * len(sig_names),\n        sig_name=sig_names,\n        p_signal=stacked,\n        fmt=[FMT] * len(sig_names),\n        adc_gain=[ADC_GAIN] * len(sig_names),\n        baseline=[BASELINE] * len(sig_names)\n    )\n    np.savez(os.path.join(output_dir, f\"{record_id}.npz\"), signal=stacked, leads=sig_names)\n    print(f\"Saved WFDB and NPZ for {record_id} -> {output_dir}\")\n\n\ndef plot_overlay(image_path: str, mask_array: np.ndarray, lead_signals: Dict[str, np.ndarray], record_id: str):\n    fig, axes = plt.subplots(4, 4, figsize=(14, 10))\n    img = Image.open(image_path)\n    axes = axes.flatten()\n    axes[0].imshow(img, cmap='gray')\n    axes[0].imshow(mask_array, alpha=0.3)\n    axes[0].set_title(\"Image + mask\")\n    lead_names = list(LEAD_LABEL_MAPPING.keys())\n    for i, lead in enumerate(lead_names, start=1):\n        ax = axes[i]\n        sig = lead_signals.get(lead)\n        if sig is None:\n            ax.text(0.5, 0.5, 'Missing', ha='center', va='center')\n        else:\n            ax.plot(sig)\n        ax.set_title(lead)\n        ax.axis('off')\n    plt.tight_layout()\n    overlay_path = os.path.join(OUTPUT_DIR, f\"{record_id}_overlay.png\")\n    plt.savefig(overlay_path)\n    plt.close(fig)\n    print(f\"Saved overlay -> {overlay_path}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Main pipeline\n1. Scan input images\n2. Rotate via Hough\n3. Batch nnU-Net prediction\n4. Cut masks and vectorize\n5. Save WFDB/NumPy (and optional overlays)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def load_mask(mask_path: str) -> np.ndarray:\n    if mask_path.endswith('.npz'):\n        data = np.load(mask_path)\n        if 'seg' in data:\n            return data['seg']\n        if 'pred' in data:\n            return data['pred']\n        if len(data.files) == 1:\n            return data[list(data.files)[0]]\n        raise ValueError(f\"Unknown npz structure in {mask_path}\")\n    img = nib.load(mask_path)\n    return np.asanyarray(img.get_fdata()).squeeze()\n\n\ndef collect_images(input_dir: str, image_ext: str) -> List[str]:\n    files = sorted(glob.glob(os.path.join(input_dir, f\"*.{image_ext}\")))\n    if DEBUG:\n        files = files[:DEBUG_MAX_IMAGES]\n    print(f\"Found {len(files)} image(s) in {input_dir}\")\n    return files\n\n\ndef prepare_rotated_images(image_paths: List[str]) -> Dict[str, Dict]:\n    meta = {}\n    for img_path in tqdm(image_paths, desc=\"Rotating\"):\n        record_id = Path(img_path).stem\n        img = cv2.imread(img_path)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        angle = get_rotation_angle(gray)\n        rotated = rotate_image(img, angle)\n        out_path = os.path.join(TMP_ROTATED_DIR, f\"{record_id}.png\")\n        cv2.imwrite(out_path, rotated)\n        meta[record_id] = {\n            \"rotated_path\": out_path,\n            \"height\": rotated.shape[0],\n            \"width\": rotated.shape[1],\n            \"angle\": angle\n        }\n        print(f\"{record_id}: rotation {angle:.2f}° -> {out_path}\")\n    return meta\n\n\ndef map_masks_to_records(masks_dir: str) -> Dict[str, str]:\n    mask_files = glob.glob(os.path.join(masks_dir, \"*.nii\")) + glob.glob(os.path.join(masks_dir, \"*.nii.gz\")) + glob.glob(os.path.join(masks_dir, \"*.npz\"))\n    mapping = {}\n    for mf in mask_files:\n        stem = Path(mf).stem.replace('.nii', '').replace('.npz', '')\n        mapping[stem] = mf\n    print(f\"Collected {len(mapping)} mask files\")\n    return mapping\n\n\ndef run_pipeline():\n    images = collect_images(INPUT_IMAGES_DIR, IMAGE_TYPE)\n    if not images:\n        raise RuntimeError(\"No images found. Update INPUT_IMAGES_DIR.\")\n    meta = prepare_rotated_images(images)\n    run_nnunet_predict(TMP_ROTATED_DIR, TMP_MASKS_DIR, MODEL_FOLD_DIR_DEFAULT, NNUNET_RESULTS_ROOT_DEFAULT, device=\"gpu\")\n    mask_map = map_masks_to_records(TMP_MASKS_DIR)\n    for record_id, info in meta.items():\n        mask_file = mask_map.get(record_id)\n        if mask_file is None:\n            warnings.warn(f\"Mask missing for {record_id}\")\n            continue\n        mask = load_mask(mask_file)\n        lead_masks = cut_to_mask(mask, LEAD_LABEL_MAPPING)\n        lead_signals = vectorise_mask(lead_masks, info[\"height\"])\n        save_wfdb(record_id, lead_signals, OUTPUT_DIR)\n        if SAVE_OVERLAY:\n            plot_overlay(info[\"rotated_path\"], mask, lead_signals, record_id)\n\nrun_pipeline()\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}